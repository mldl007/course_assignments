{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fee1a69f-ba76-43d6-9cb9-f2a72772e98e",
   "metadata": {},
   "source": [
    "<h4>1. Explain the Activation Functions in your own language a) sigmoid b) tanh c) ReLU d) ELU e) LeakyReLU f) swish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773975a2-8016-43c6-be9a-8fc0e64abf59",
   "metadata": {},
   "source": [
    "a) sigmoid activation function restricts the input values between 0 and 1.\n",
    "b) tanh activation function restricts the input values between -1 and 1.\n",
    "c) ReLU activation function restrict all values <0 to 0 and passes all positive values as is.\n",
    "d) Exponential Linear Unit or its widely known name ELU is a function that tend to converge cost to zero faster and produce more accurate results\n",
    "e) Leaky ReLU is similar to ReLU but instead of restricting negative values to 0, it converts them to 0.01x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c148fdfd-8d3a-486e-9386-ca2b8cc50473",
   "metadata": {},
   "source": [
    "<h4>2. What happens when you increase or decrease the optimizer learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e25c3-47fc-4c6a-93ce-2381e14632cd",
   "metadata": {},
   "source": [
    "When you increase the learning rate the gradient varies heavily leading to poor performance. When we reduce the learning rate the convergence may take a very long time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e03eb9-18cd-4ebd-8494-fd9bb8b41722",
   "metadata": {},
   "source": [
    "<h4>3. What happens when you increase the number of internal hidden neurons?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bcdf7d-716a-4e16-9e64-8f4c5f1d856d",
   "metadata": {},
   "source": [
    "Increasing the number of neurons lead to increase in number of parameters in the network and may lead to overfitting and increased training time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b930e36e-f8ad-4f9b-9717-1389a5e70470",
   "metadata": {},
   "source": [
    "<h4>4. What happens when you increase the size of batch computation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be94470f-9dfa-4cc8-9a97-367cd80c5818",
   "metadata": {},
   "source": [
    "When the batch size is increased the number of samples in an epoch increases. If batch size is very huge, there may be issues with memory and computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b178856-24db-4860-94fe-7289535a84ab",
   "metadata": {},
   "source": [
    "<h4>5. Why we adopt regularization to avoid overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac246c1c-0551-44db-9fd5-efede961ffc2",
   "metadata": {},
   "source": [
    "Regularization constaints the weights from growing higher thereby preventing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950ed2c4-a092-40af-9a77-6965e3d42128",
   "metadata": {},
   "source": [
    "<h4>6. What are loss and cost functions in deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6587f5-9501-46fa-a581-e5c48e4982b5",
   "metadata": {},
   "source": [
    "Loss functions and cost functions enable the optimazation algorithm to compute the gradient and decide how by how much the weights must be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0973805-8f4f-44d8-a36e-9670d4cf4599",
   "metadata": {},
   "source": [
    "<h4>7. What do you mean by underfitting in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a25b9-ad6d-4bac-822e-a54f147a7d0c",
   "metadata": {},
   "source": [
    "Underfitting means the network is unable to find patterns in data. This may be due to very high learning rate, very low learning rate otr the network architecture is too simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b682d27d-230b-4e08-a505-a2a16ba1ec31",
   "metadata": {},
   "source": [
    "<h4>8. Why we use Dropout in Neural Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f984df9-8851-4e63-ac1b-4fcb42472b3a",
   "metadata": {},
   "source": [
    "Dropouts randomly drop a specified percentage of neurons in a layer and the weights of those neurons are not updated in the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c40583a-1442-4e64-80eb-fcf70e72746b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
