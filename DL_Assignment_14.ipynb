{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f29e7798-6eaa-47a4-8838-74bf273c663c",
   "metadata": {},
   "source": [
    "<h4>1. Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63325ea4-4af8-4945-b35b-07e900090dbe",
   "metadata": {},
   "source": [
    "Initializing all weights to same values causes symmetry proble. The output of neurons in a fully connected layer will be same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecbae25-dce7-4dac-ac56-4ec44db988fe",
   "metadata": {},
   "source": [
    "<h4>2. Is it okay to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec885007-b649-40b9-be7c-046eaa5cae98",
   "metadata": {},
   "source": [
    "It is ok to initialize the bias to 0 as long as weights are not 0, since the model updates bias during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6099aac1-946c-451d-b218-21a4caf62ab4",
   "metadata": {},
   "source": [
    "<h4>4. In which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76e5d3b-b4b0-4f39-813e-81543e58ee34",
   "metadata": {},
   "source": [
    "Relu and its variants are used in hidden layers. Sigmoid and Tanh are used in LSTM and RNN. Sigmoid is also used in output layer of a binary classification problem. Softmax is used in the output layer of multi class classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8607761f-5d14-446d-8104-7c07787a3807",
   "metadata": {},
   "source": [
    "<h4>5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3481e5-b9dc-4883-bd2a-3dcc5d30789b",
   "metadata": {},
   "source": [
    "If we set the momentum to 1, then the (1-alpha) value of EWMA used for eliminating noise from gradients becomes 0. Hence, the historical gradient values are not considered while smoothening/remove noise from the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded8688-7b01-4882-b124-1455642bfd28",
   "metadata": {},
   "source": [
    "<h4>7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58853c2-43d7-40ac-bf7a-c082ff39d8a0",
   "metadata": {},
   "source": [
    "Dropout may slow down training since few neurons are elimunated from weight updation. It may take time for model to learn the patterns fully. It has no effect during inference, since the weights are just multiplied by dropout percentage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
