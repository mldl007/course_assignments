{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f1cd4ac-6425-40bc-bfda-0ffcfb455dad",
   "metadata": {},
   "source": [
    "<h4>1. Is there any way to combine five different models that have all been trained on the same training\n",
    "data and have all achieved 95 percent precision? If so, how can you go about doing it? If not, what is\n",
    "the reason?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e629d9-56ba-4e0e-9a07-b8b285c898ee",
   "metadata": {},
   "source": [
    "We can use ensemble learning technique to combine multiple models trained on same data. VotingClassifier/Regressor is an option in Scikit-Learn to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ab3b6-8250-42f4-aef3-333b80f57b52",
   "metadata": {},
   "source": [
    "<h4>2. What&#39;s the difference between hard voting classifiers and soft voting classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400f15cf-92a5-41a2-9a60-09a4b9cbd1d7",
   "metadata": {},
   "source": [
    "hard voting used predicted classes to output the final prediction. Whereas, soft voting used prediction probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1579e4-6e2c-4f21-8502-26647a580118",
   "metadata": {},
   "source": [
    "<h4>3. Is it possible to distribute a bagging ensemble&#39;s training through several servers to speed up the\n",
    "process? Pasting ensembles, boosting ensembles, Random Forests, and stacking ensembles are all\n",
    "options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfac070-887c-4add-9428-ab588dcc31f3",
   "metadata": {},
   "source": [
    "Yes it is possible to distribute a bagging ensemble's training through several servers to speed up the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192afed1-70ad-4ca9-9696-9882e89ec8dc",
   "metadata": {},
   "source": [
    "<h4>4. What is the advantage of evaluating out of the bag?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48dfed6-6a46-4385-a51b-4b2660ab49bc",
   "metadata": {},
   "source": [
    "OOb evaluation is a technique used in random forests where the samples that are not bootstraped for training are used for evaluating each tree in the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053691d9-f39e-461c-b90d-5a8c049d0af8",
   "metadata": {},
   "source": [
    "<h4>6. Which hyperparameters and how do you tweak if your AdaBoost ensemble underfits the training\n",
    "data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a7a447-be32-47f0-a6f8-0ea42379d2c5",
   "metadata": {},
   "source": [
    "Increase the learning rate and  n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c124aa-cc55-4f4c-97d4-2aec67a30b93",
   "metadata": {},
   "source": [
    "<h4>7. Should you raise or decrease the learning rate if your Gradient Boosting ensemble overfits the\n",
    "training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2718a956-3f0f-4383-995e-2eb9cae6bd7f",
   "metadata": {},
   "source": [
    "Reduce the learning rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
